---
title: "CUNY MSDA 607 Project 2 (Week 7)"
author: "James Topor"
date: "March 2, 2016"
output: 
    html_document:
        toc: true
        depth: 3
        number_sections: FALSE
        theme: spacelab
        highlight: tango
---
```{r, echo = FALSE, message=FALSE, warning=FALSE}
# load required packages
library(tidyr, warn.conflicts = FALSE, quietly=TRUE)
library(dplyr, warn.conflicts = FALSE, quietly=TRUE)
library(stringr)
library(knitr, warn.conflicts = FALSE, quietly=TRUE)
library(ggplot2)
```

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

# Introduction

For this project we are asked to choose any three of the “wide” datasets identified in the Week 6 Discussion items. For each of the three chosen data sets we were asked to:

- Create a .CSV file that includes all of the information included in the dataset, preferably using a "wide" format that can be "tidied";

- Read the information from the .CSV files into R, and use the __tidyr__ and __dplyr__ packages as needed to tidy and transform the data;

- Perform the analysis requested in the discussion item.

The three data sets selected are as follows:

- __Oil Purchase and Consumption Data__    (*submitted by Kishore Prasad*)

- __Untidy MLB World Series Data__    (*submitted by Logan Thomson*) 

- __County Data__    (*submitted by James Topor*)

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

# Data Set 1: Oil Purchase and Consumption Data  

A .csv file was created for the oil purchase and consumption data using Excel. That .csv file was then loaded onto the Github.com website for purposes of reproducibility. Once the data has been loaded and tidied, we are asked to derive the following metrics:

- Give the closing balance of component + Brand

- Give most consumed brand across the 2 categories of oil. 

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Exploring the Oil Purchase and Consumption Data

We'll start by loading the **tidyr** and **dplyr** packages and reading and displaying the unitidy data from the .csv file. The .csv file itself is located at a publically accessible Github.com page:

```{r, message=FALSE, warning=FALSE}
oil_df <- read.csv("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-607/master/Project%202/OilDatacsv.csv", header = TRUE, stringsAsFactors = FALSE)
kable(oil_df, padding = 0)
```

The data set appears to be comprised of a table that shows the amounts of Engine Oil and GearBox Oil purchased and consumed each month for three different brands of oil: Caltex, Gulf, and Mobil. We are told that the numbers provided in columns 3 through 5 'indicate "Purchased : Consumed" in Gallons'.

The display of the untidy data shows that while the data have an obvious structure, there are several characteristics we need to be concerned with:

1. The names of months are provided in the first column, but are not enumerated across the entirety of the rows they apply to. For example, the third row of data lists 'Jan' as the month that the data in that row pertain to. However, the data in the fourth row also appear to pertain to 'Jan' but that row lacks any actual identifier for a month name. This creates an issue when attempting to transform the data using R's tidying functions since those functions expect the columns and rows they are working with to hold a complete set of attributes for each of the relevant variables.

2. The first two rows have a month identifier of "Open", which is obviously not the name of any month. While the meaning of the term "Open" was not explained to us, we can postulate that it is meant to represent the supply amounts for the various types of oil listed in columns 3 through 5 at the start of whatever year this data was recorded for. Furthermore, since the word "Open" is not an actual month name, we will not be able to make use of R's native month name sorting functionality: any attempt to sort by month name will necessarily result in an inaccurate result.

3. Columns 3 through 5 each contain two separate metrics separated by a colon character (':'). We were told that the first metric indicates gallons of oil purchased for the type of oil specified in the column heading while the second metric indicates gallons of oil consumed for that same type of oil. Having two separate metrics embedded within a single column element obviously violates the principles of "tidy data".

We'll need to address each of these items prior to any attempt to calculate the requested metrics. 

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Tidying the Oil Purchase and Consumption Data

We'll start by filling in the missing month names:
```{r}
# fill in missing month names in data
for(i in seq(from = 2, to = nrow(oil_df), by=2) ) {
  oil_df$Month[i] = oil_df$Month[i - 1]
}
kable(oil_df, padding = 0)
```

To address issue 2 from above, we'll add an index column that is independent of the month names but that replicates their original order within the data. This will allow us to resort the data in such a way that the month order is maintained despite the presence of the "Open" identifier:
```{r}
oil_df$index <- as.numeric(row.names(oil_df), length = 2)
```


Next, we'll gather the three separate oil brands (Caltex, Gulf, and Mobil) into a single column named 'Brand' and move their corresponding "Purchased/Consumed" values to new column named "PurchConsume":
```{r}
oil_df2 <- gather(oil_df, Brand, PurchConsume, -Month, -Category, -index, na.rm = TRUE)
kable(oil_df2, padding = 0)
```

Transforming the data in this manner has left us with a data frame that is sorted by 'Brand' rather than by 'Month'. If we'd prefer to have the data sorted by 'Month' identifier we can sort using the index values we created above so that we end up with the "Month" column sorted in such a manner that the "Open" identifier appears first followed by a chronological ordering of the month names (as was the case with the original .csv data). 

We'll make use of R's **order** function for that purpose and then separate the 'PurchConsume' column into 2 new columns labeled "Purchased" and "Consumed". This will allow us to analyze these individual metrics independent of one another further on:
```{r}
# sort the data using the 'index' column values we created earlier to ensure proper ordering of 'Month'
# column
oil_df3 <- oil_df2[order(oil_df2$index), ]

# separate the 'PurchConsume' column into two new columns named 'Purchased' and 'Consumed'
oil_df4 <- separate(oil_df3, PurchConsume, c("Purchased", "Consumed"), sep = ":")
kable(oil_df4, padding = 0)
```

As we can see, the resulting data frame is tidy: each column contains values for a single variable, and each row represents a single observation (Month; Category/Type of oil; Brand; amount purchased during the month; amount consumed during the month). The data are ordered first by 'Month', then by 'Category'. Now that we've completed our "tidying" efforts we are free to move on to our analysis of the Oil data.

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Analyzing the Oil Purchase and Consumption Data

For the analysis of the Oil data, we were asked to derive the following metrics:

- Give the closing balance of component (aka 'Catgory') + Brand

- Give most consumed brand across the 2 categories of oil. 


To find the closing balance of each category/Brand pair, we can make use of R's __group_by__ and __summarise__ functions. First we use the __group_by__ function to consolidate the rows by 'Brand' and 'Category'. Then we apply the __summarise__ function to calculate the sum totals of each month's purchases and consumption (including the "Open" amounts), and subtract the total amount consumed from the total amount purchased. The final result is our closing balance for each 'Category' of oil:

```{r}
oil_a1 <- data.frame(summarise(group_by(oil_df4, Brand, Category), 
                     TotalPurchased = sum(as.numeric(Purchased)), 
                     TotalConsumed = sum(as.numeric(Consumed)),
                     ClosingBalance = TotalPurchased - TotalConsumed))
kable(oil_a1, padding = 0)
```


We can find the most consumed brand for each category of oil using R's __subset__ and __arrange__ functions. First we subset the data by the 'Category' of oil. Then we sort the results in descending order of 'TotalConsumed'. Let's start with the 'Category' of 'Engine Oil':
```{r}
kable(arrange(subset(oil_a1, Category == 'Engine Oil'), desc(TotalConsumed)), padding = 0 )
```

Our results show that Gulf had the most consumed 'Engine Oil' during the period covered by the data.

Now let's try 'GearBox Oil':
```{r}
kable(arrange(subset(oil_a1, Category == 'GearBox Oil'), desc(TotalConsumed)), padding = 0 )
```

Caltex had the most consumed 'GearBox Oil' during the period covered by the data. 

We can also present these results graphically:

```{r, echo=FALSE}
# create vectors containing ONLY TotalConsumed figures
x <- filter(oil_a1, TotalConsumed, Brand == 'Caltex')$TotalConsumed
y <- filter(oil_a1, TotalConsumed, Brand == 'Gulf')$TotalConsumed
z <- filter(oil_a1, TotalConsumed, Brand == 'Mobil')$TotalConsumed

# create a two row matrix with x and y
height <- rbind(x, y, z)

# Plot the TotalConsumed figures side-by-side
colors <- c("darkblue", "red", "black")

mp <- barplot(height, beside = TRUE, 
              ylim = c(0, 950), 
              names.arg = unique(oil_a1$Category),
              main="Oil Consumption Amount by Category, Jan-June",
              ylab="Amount Consumed",
              col = colors,
              args.legend = list(x = "topright", cex = .5) )

# format the legend
legend("topright", 
       inset=c(0,0),
       ncol = 1,
       cex = .7,
       legend = c("Caltex", "Gulf", "Mobil"), 
       fill = c("darkblue", "red", "black"))

# write the percentage values above the individual bars in the plot
text(mp, height, labels = format(height, 4),
     pos = 3, cex = .6)
```

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

# Data Set 2:  Untidy MLB World Series Data  

A .csv file was created for the MLB World Series data using Excel. That .csv file was then loaded onto the Github.com website for purposes of reproducibility. Once the data has been loaded and tidied, we are asked to derive the following metrics:

*"..find out the teams with the most World Series appearances, those with the least, who has won the most Series games, won the least, what team has the highest percentage of series wins per World Series appearances.."*

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Exploring the MLB World Series Data

Let's load and display the untidy data:
```{r}
ws_df <- read.csv("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-607/master/Project%202/WSBBcsv.csv",
                  header = TRUE, stringsAsFactors = FALSE)
kable(ws_df, padding = 0)
```

The display of the untidy data shows that while the data have an obvious structure, there are several characteristics we need to be concerned with:

1. While we have column headers for each of the three columns, those headers reappear within the actual data in rows 17, 28, and 30.

2. In row 23 we find an entry for which there was no corresponding World Series data to report since a World Series wasn't held in that year (1994).

3. The third column (labeled 'Results') contains at least four separate pieces of information within each column element: The name of the winning team; a count of how many World Series games the winning team won that year; the name of the losing team; and a count of how many World Series games the losing team won that year.

We'll need to address each of these items prior to any attempt to calculate the requested metrics.

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Tidying the MLB World Series Data

To address items 1 and 2 from above, we'll blank out the interspersed instances of the column header, blank out all items for the year in which the World Series wasn't held, and then remove all rows from the data that lack data:

```{r}
# Blank out interspersed column header items:
ws_df$Year[ws_df$Year == "Year"] = NA
ws_df$Results[ws_df$Results == "Results"] = NA
ws_df$MVP[ws_df$MVP == "MVP"] = NA

# Blank out items for "Not Held" years
ws_df$MVP[ws_df$Results == "Not Held"] = NA
ws_df$Results[ws_df$Results == "Not Held"] = NA

# Remove rows that don't have any data in them:
ws_df <- na.omit(ws_df)
kable(ws_df, padding = 0)
```

To address the third item listed above we'll need to make use of __tidyr__'s __separate__ function. First we'll split the 'Results column into two new columns named 'Winner' and 'Loser'. This gets us halfway toward resolving our '4 data values in 1 column' problem.

Then each of those new columns will need to be separated into 2 new columns so that we have the team names and the number of games won in individual columns. For the winning team the new columns will be named 'WinTeam' and 'WT Gwon', while for the losing team the new columns will be named 'LoseTeam' and 'LT Gwon'. 

Finally, we'll use __dplyr__'s __mutate__ function to create a new column that represents the total number of World Series games played during a specific year. That total is found by simply summing the total number of games won by the winning and losing teams for the specific year.

```{r, warnings=FALSE, messages=FALSE, error=FALSE}
# Separate Results column into two new columns: 'Winner' and 'Loser':
ws_df2 <- separate(ws_df, Results, c("Winner", "Loser"), sep = ",", stringsAsfactors = FALSE)

#Now separate 'Winner' column into 'WinTeam' and 'WT_GWon'
ws_df3 <- separate(ws_df2, Winner, c("WinTeam", "WT_GWon"), sep = -2, stringsAsfactors = FALSE)

# trim blank spaces from winning team name
ws_df3$WinTeam = trimws(ws_df3$WinTeam)

# Separate 'Loser' column into 'LoseTeam' and 'LT_GWon'
ws_df4 <- separate(ws_df3, Loser, c("LoseTeam", "LT_GWon"), sep = -2, stringsAsfactors = FALSE)

# trim blank spaces from losing team name
ws_df4$LoseTeam = trimws(ws_df4$LoseTeam)

#Add a column that indicates the total number of World Series games played for a given year
ws_df5 <- data.frame(mutate(ws_df4, TotalGames = as.numeric(WT_GWon) + as.numeric(LT_GWon)) )

# display the 'tidied' data frame
kable(ws_df5, padding = 0)
```

We now have a 'tidied' version of the original data that is supplemented by a single additional column containing a derived metric (total games played during the World Series that year). Therefore, we are ready to proceed with the requested analysis.

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Analysis of the MLB World Series Data

The discussion posting for this data set suggested the following types of analysis:

*"..find out the teams with the most World Series appearances, those with the least, who has won the most Series games, won the least, what team has the highest percentage of series wins per World Series appearances.."*

We'll attempt to derive each of these metrics using tools such as R's __table__, __merge__, __summarise__, and __arrange__ functions.

### Most and Least World Series Appearances  
To find the teams with the most and least World Series appearances we'll apply R's __table__ function to both the 'WinTeam' and 'LoseTeam' columns of our data frame. The __table__ function will generate a table containing the team names and the number of times they won the World Series as indicated in the original data set. 

Since we need to apply the __table__ function against two separate columns, we'll have two separate interim data frames: one containing the names of the various winning teams and the number of times they've won the World Series during the period covered by the original data; and one containing the names of the various losing teams and the number of times they've lost the World Series during the period covered by the original data. Those two interim data frames are then combined via R's __merge__ function to create a single consolidated data frame that can tell us how many winning and losing appearances the various teams had. 

To calculate the overall number of winning and losing appearances we then apply R's __group_by__ and __summarise__ functions such that for each team listed within the consolidated data frame we sum their winning and losing appearance totals:
```{r}
# get the counts of winning team appearances
df1 <- as.data.frame(table(ws_df4$WinTeam), stringsAsFactors = FALSE)

# get the counts of losing team appearances
df2 <- as.data.frame(table(ws_df4$LoseTeam), stringsAsFactors = FALSE)

# merge the two data frames
df3 <- merge(df1, df2, all.y = T, all.x = T, sort= T, stringsAsfactors = FALSE)

# summarize by team name, summing the number of winning and losing appearances
df4 <- data.frame(summarise(group_by(df3, Var1), Appearances = sum(Freq)))

colnames(df4)[1] <- "TeamName"

# sort by descending order of number of appearances
# kable(arrange(df4, desc(Appearances)), padding=0 )

# sort by descending order of games won
top_appr <- arrange(df4, desc(Appearances))

ggplot(top_appr, aes(x=Appearances, y=reorder(TeamName, Appearances))) +
  xlim(0, 8) +
  geom_segment(aes(yend=TeamName), xend=0, colour="grey50") +
  geom_point(size=3, colour = "firebrick") +
  theme_bw() +
  theme(panel.grid.major.y = element_blank() )

```

While we now have the team identifiers ordered by total number of appearances, we can see from the list that the data do not maintain consistent identifiers for the teams that either won or lost the World Series: multiple teams are listed using more than one identifier. For example:

- The Boston Red Sox can be found using the values "Boston" or "Red Sox";
- The New York Yankees can be found using the values "NY Yankees" or "Yankees";
- The Detroit Tigers can be found using the values "Detroit" or "Tigers";
- The Cardinals are identified by "St. Louis" or Cardinals", etc.

Given the inconsistency in the naming of the teams it isn't feasible to attempt the analysis suggested for this data set without performing a significant amount of 'manual' work to ensure that the team identifiers are consistent throughout the data set. Such work is beyond the scope of this analysis.

Furthermore, it should be noted that the "Least Number of World Series Appearances" metric is a bit misleading since many Major League Baseball teams never made any appearances in the World Series during the years documented by this data set. Therefore, the "least" number derived here applies solely to those teams that actually appeared in the World Series for the years documented by this data set.

### Most and Least World Series Game Wins & Team's Overall Winning Percentage  
We can attempt to find the teams with the most and least World Series game wins and each team's overall winning percentages following a methodology similar to that which we used above for the "Most Appearances" analysis, keeping in mind the issue we've identified with the inconsistent identifiers used for teams within the data set.

We start by creating two temporary data frames containing the team name, number of games won, and total games played for both the winning an losing teams for any given year. We then merge those two interim data frames.  

After creating the merged data frame we apply the __group_by__ and __summarise__ functions to derive the required winning percentages for each team. We first calculate the total number of games won by summing the individual "GamesWon" entries for each team. For completeness purposes, we also calculate the total number of games lost by subtracting the total number of games won from the total games played. Finally, we calculate the winning percentage by dividing the total games won by the total number of games played. When this derivation work is complete, we display the results sorted in descending order by the number of games won.
```{r}
# get winning team names and games won
df1 <- data.frame(ws_df5$WinTeam, as.numeric(ws_df5$WT_GWon), ws_df5$TotalGames, stringsAsFactors = FALSE)
# rename cols 1 and 2 to meaningful names: Airline and ArrDelay
colnames(df1)[1] <- "TeamName"
colnames(df1)[2] <- "GamesWon"
colnames(df1)[3] <- "TotalGames"

# get losing team names and games won
df2 <- data.frame(ws_df5$LoseTeam, as.numeric(ws_df5$LT_GWon), ws_df5$TotalGames, stringsAsFactors = FALSE)
# rename cols 1 and 2 to meaningful names: Airline and ArrDelay
colnames(df2)[1] <- "TeamName"
colnames(df2)[2] <- "GamesWon"
colnames(df2)[3] <- "TotalGames"

# merge the two data frames
df3 <- merge(df1, df2, all.y = T, all.x = T, sort= T, stringsAsfactors = FALSE)

# summarize by team name, summing all games won
df4 <- data.frame(summarise(group_by(df3, TeamName), GamesWon = sum(GamesWon), 
                            GamesLost = sum(TotalGames)- GamesWon,
                            WinPercentage = GamesWon/sum(TotalGames) ) )

# sort by descending order of games won
top_games <- arrange(df4, desc(GamesWon))

# graph results
ggplot(top_games, aes(x=GamesWon, y=reorder(TeamName, GamesWon))) +
  geom_segment(aes(yend=TeamName), xend=0, colour="grey50") +
  geom_point(size=3, colour = "firebrick") +
  theme_bw() +
  theme(panel.grid.major.y = element_blank() )

```

The NY Yankees appear to be the team from this data set that have won the most total World Series games while four teams (Colorado, Houston, San Diego, and the Tigers) hadn't won any. However, as mentioned earlier, there is a lack of consistency in this data set's use of team identifiers. If we peruse the sorted list we've generated we see that the NY Yankees are also identified via the term "Yankees". Similar issues arise with several other teams (e.g., the Boston Red Sox, the San Francisco Giants, etc.), so at best the counts we've calculated are accurate only in the sense that they are relative to the inconsistent use of team identifiers within the original data set.

A team's overall winning percentage was calculated by dividing the total number of World Series games it won by the total number of World Series games the team played in. Let's see which teams had the highest and lowest overall winning percentages:
```{r}
# sort by winning percentage
kable(arrange(df4, desc(WinPercentage)), padding = 0)
```

Three teams appear to have won 100% of the World Series games they played in. However, once we take into consideration the issue we found earlier regarding a lack of consistency in the use of team identifiers, we see that the Boston Red Sox actually had not won 100% of their games: entries encoded with "Boston" show no losses, but there is also an entry under "Red Sox" which shows a record of 4 wins and 2 losses. Several other team's winning percentages are similarly affected by the inconsistent use of team identifiers. Therefore, the most we can say about the winning percentages we've calculated is that they are accurate relative to the set of inconsistent team identifiers provided in the data set.

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

# Data Set 3:  County Data

A .csv file was created for the County data using Excel. That .csv file was then loaded onto the Github.com website for purposes of reproducibility. Once the data has been loaded and tidied, we are asked to determine the following:

*Determine whether number of college educated residents in the county (and possibly either of the other the two fixed variables) is a predictor of the number of jobs in the county.*

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Exploring the County Data

Let's load and display the untidy data:
```{r}
c_df <- read.csv("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-607/master/Project%202/Countydata.csv", header = TRUE, stringsAsFactors = FALSE)
kable(c_df, padding = 0)
```

The display of the untidy data shows that the primary characteristic we need to be concerned with for data analysis purposes is that the rows of data contain multiple observations for multiple years for two separate variables. The two variables are:

- The proportion of college educated residents within a county

- The total number of jobs in the county

Our attempts at rearranging the data will need to ensure that those multiple observations and variables are transformed into a format that limits the data contained within any given row to the relevant variable observations for a single year and a single county. Organizing the data in such manner will then allow us to attempt the requested analysis.

Columns 1 through 3 each contain 'static' data that does not change over time:

- County Name
- Land Area: The gross land area in square miles for the given county
- Natural Amenity: A boolean indicator whose meaning is not explained; We are told a value of 4 = 'Yes' while a value of 3 = 'No'

Since the values of these three variables do not change over time, we won't need to be concerned with tidying or transforming them as part of our work here.

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Tidying the County Data

This data set was extracted from http://www.theanalysisfactor.com/wide-and-long-data/. That web page provides an example of how the County data could be transformed into a "Long" format. Our goal in tidying the data we've loaded is to have it conform to the 'Long' format shown on that web page.

We'll start by creating a temporary data frame containing the 'County', 'LandArea', 'NatAmenity', and 'College----' columns and using __tidyr__'s __gather__ function to consolidate the 'College----' column headings into a new column named 'Year' while converting the corresponding observations to values within a new column named 'College'. We then sort the results by 'County' and strip the values within the new 'Year' column of their 'College' prefixes.

```{r}
# select everything EXCEPT the jobs data from the original data frame
college_df <- select(c_df, County, LandArea, NatAmenity, College1970, College1980, College1990, College2000)
kable(college_df, padding = 0)

# gather the 'College' columns
c_df2 <- gather(college_df, Year, College, College1970:College2000, na.rm = TRUE)
# c_df2

# sort the data so it is ordered by county again
c_df2 <- arrange(c_df2, County)

# clean the "Year" column of the word 'College'
c_df2$Year <- str_extract(c_df2$Year, "[[:digit:]]{4}")

kable(c_df2, padding = 0)
```

Now we'll create a second temporary data frame using this same approach but applied to the 'Jobs----' columns:

```{r}
# select everything EXCEPT the jobs data from the original data frame
jobs_df <- select(c_df, County, LandArea, NatAmenity, Jobs1970, Jobs1980, Jobs1990, Jobs2000)
kable(jobs_df, padding = 0)

# gather the 'Jobs' columns
j_df2 <- gather(jobs_df, Year, Jobs, Jobs1970:Jobs2000, na.rm = TRUE)
# j_df2

# sort the data so it is ordered by county again
j_df2 <- arrange(j_df2, County)

# clean the "Year" column of the word 'Jobs'
j_df2$Year <- str_extract(j_df2$Year, "[[:digit:]]{4}")

kable(j_df2, padding = 0)
```

We now have both the college data and the jobs data in tidy format and arranged in identical patterns. The rows of each data frame are ordered by county name, then by year. All that's left to do is attach the 'Jobs' column containing the job counts within our temporary jobs data frame to the data frame containing the college data:

```{r}
c_df2$Jobs <- j_df2$Jobs
kable(c_df2, padding = 0)
```

The County Data is now arranged in 'Long' format identical to that shown at http://www.theanalysisfactor.com/wide-and-long-data/. The 'Long' format serves as the basis for our analysis in the following section.

__---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__

## Analysis of the County Data

The discussion posting for this data set suggested the following types of analysis:

*Determine whether number of college educated residents in the county (and possibly either of the other the two fixed variables) is a predictor of the number of jobs in the county.*

We can use R's __lm__ linear modeling function to assess whether the number of college educated residents is, in fact, predictive of the number of jobs within a county. We'll do this across all five of the counties as well as for each county individually.

We can state our explanatory and response variables for these regression tests as:

- Explanatory Variable: Proportion of College Educated Residents Within a County
- Response Variable: Number of Jobs that Exist Within a County

A hypothesis test for whether the explanatory variable is a valid predictor of the response variable can be stated as:

__H0: $\beta$ = 0__: The true linear model of the relationship between the explanatory and response variables has a slope of zero.

__HA: $\beta$ != 0__: The true linear model of the relationship between the explanatory and response variables has a slope of other than zero.

### College Educated Residents as a Predictor of Jobs: Across All Counties

Let's start by applying the regression model across all of the counties for all of the years in the data set and generating a scatter plot with the regression line overlayed:  

```{r, echo=FALSE}
# fit a model & plot
fit1 <- lm(Jobs ~ College, data = c_df2)

# plot a scatter plot and overlay the regression line
plot(c_df2$Jobs ~ c_df2$College, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates')
abline(fit1)
```

Our plot shows a very clear positive relationship between the proportion of college educated residents living in a county and the number of jobs that are present within a county.

We can use R's native __cor__ function to calculate the correlation coefficients of our explanatory / response variables:  
```{r}
cor(c_df2$College, c_df2$Jobs)
```

We have a strong 0.78 correlation between the two variables. This provides some validation of what we observed in the scatter plot above.

Applying R's __summary__ function to the output of the line fitting function allows us to further examine the robustness of the relationship between the two variables:
```{r}
summary(fit1)
```

From the above, the characteristic equation for the estimated number of jobs within a county given the proportion of college graduates within the county is:  

$$yhat = -5604 + 223643 * (Proportion Of College Educated Residents)$$  

and the p value at a 95% confidence level is approximately zero. Given the large coefficient for the explanatory variable and the p-value of approximately zero, we reject the null hypothesis (H0) and conclude that there is, in fact, a tangible relationship between the proportion of college educated residents within a county and the number of jobs that are present within a county.

Let's look at the individual counties now.

###  College Educated Residents as a Predictor of Jobs: Within the Individual Counties

Displaying our tidied data again here we can see that we have only four observations for our explanatory and response variables for each county. Given this small number of responses, it is unlikely that any attempt to apply linear regression to the data for each specific county will yield a statistically valid result. In general, it is recommended that we have at least 10 observations for each variable if we wish to apply linear regression modeling. Therefore, we will not apply least squares line fitting to each of the individual counties.

Instead, we'll make use of linear plots of the college graduate and jobs metrics for each of the counties. This will allow us to visually assess the relationship between the two variables 

```{r, echo=FALSE}

# par(mfrow=c(3,2))
    
par(mfrow = c(2,3), oma = c(2, 2, 0, 0), mar=c(2, 2, 2, 2) + 2)

# __Autauga County__
t1 <- subset(c_df2, County == 'Autauga')

# plot a scatter plot and overlay the regression line
#qplot(t1$College, t1$Jobs, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates', 
#     main = 'Autauga County', geom = "line")

plot(t1$Jobs ~ t1$College, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates',
     main = 'Autauga County', type = "l")


# __Baldwin County__
t2 <- subset(c_df2, County == 'Baldwin')

# plot a scatter plot and overlay the regression line
#qplot(t2$College, t2$Jobs, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates', 
#     main = 'Baldwin County', geom = "line")

plot(t2$Jobs ~ t2$College, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates',
     main = 'Baldwin County', type = "l")


# __Barbour County__
t3 <- subset(c_df2, County == 'Barbour')

# plot a scatter plot and overlay the regression line
#qplot(t3$College, t3$Jobs, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates', 
#     main = 'Barbour County', geom = "line")

plot(t3$Jobs ~ t3$College, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates',
     main = 'Barbour County', type = "l")


# __Bibb County__
t4 <- subset(c_df2, County == 'Bibb')

# plot a scatter plot and overlay the regression line
#qplot(t4$College, t4$Jobs, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates', 
#     main = 'Bibb County', geom = "line")

plot(t4$Jobs ~ t4$College, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates',
     main = 'Bibb County', type = "l")


# __Blount County__
t5 <- subset(c_df2, County == 'Blount')

# plot a scatter plot and overlay the regression line
#qplot(t5$College, t5$Jobs, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates', 
#     main = 'Blount County', geom = "line")

plot(t5$Jobs ~ t5$College, ylab = 'Number of Jobs', xlab = 'Proportion of College Graduates',
     main = 'Blount County', type = "l")


par(mfrow=c(1,1))
```

In three of the five counties (Autauga, Baldwin, and Blount) there appears to be a clear positive relationship between the proportion of the population having a college degree and the number of jobs within the county. While Blount County appears to have had some job growth without a change in the number of college graduates for a portion of the time period covered, the balance of that county's data show a strong correlation between the two variables.

For Bibb County, for one of the time periods covered there appears to be an increase of jobs despite decrease in the proportion of college graduates. This phenomenon might warrant further analysis at some point: how was the county able to increase its job base by more than what appears to be 25% while seeing a decline in college graduates within the county? Was it the result of a large unskilled labor employer opening or expanding a facility? In any event, after that specific time period both variables experienced positive growth.

Barbour County also appears to have experienced a decline in the proportion of college graduates while increasing the number of jobs during one of the available time periods. As was the case with Bibb County, it might be useful to understand the underlying causes of that result.

These individual county plots have shown that while in general there is a strong positive correlation between college-educated residents and the number of jobs within a county, it isn't necessary for a county to increase its proportion of college residents for it to experience job growth. 